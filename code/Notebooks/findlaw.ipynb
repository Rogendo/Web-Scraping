{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chrome_options = Options()\n",
    "\n",
    "chrome_options.headless = True\n",
    "driver = webdriver.Chrome(options=chrome_options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_findlaw_top_blogs(url):\n",
    "    \"\"\"\n",
    "    Get the top blogs from findlaw.com\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    blogs = soup.find_all('ol', class_='blogs')\n",
    "\n",
    "    blog_list = []\n",
    "    for blog in blogs:\n",
    "        links = blog.find_all('a')\n",
    "        for link in links:\n",
    "            link = link['href']\n",
    "            blog_list.append(\n",
    "                 link\n",
    "            )\n",
    "\n",
    "    df = pd.DataFrame(blog_list)\n",
    "    return blog_list\n",
    "\n",
    "\n",
    "def get_findlaw_blogs(url):\n",
    "    \"\"\"\n",
    "    Get the blogs from findlaw.com\n",
    "    \"\"\"\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    blogs = soup.find_all('div', class_='blogItem')\n",
    "    blog_list = []\n",
    "\n",
    "    for blog in blogs:\n",
    "        # title = blog.find('h2').text\n",
    "        # <a href=\"https://www.findlaw.com/legalblogs/practice-of-law/\" title=\"Practice of Law\">Practice of Law</a>\n",
    "    #     author = blog.find('span', class_='author').text\n",
    "    #     date = blog.find('span', class_='date').text\n",
    "        link = blog.find('a')['href']\n",
    "        blog_list.append(\n",
    "            # 'title': title,\n",
    "            # 'author': author,\n",
    "            # 'date': date,\n",
    "             link\n",
    "        )\n",
    "        df = pd.DataFrame(blog_list)\n",
    "\n",
    "    return blog_list\n",
    "\n",
    "\n",
    "# def concatinated_findlaw_blogs_link_index():\n",
    "#     \"\"\"\n",
    "#     Concatinate the top blogs and blogs from https://www.findlaw.com/legalblogs/ into a single dataframe\n",
    "    \n",
    "#     param: url: str: url of the website\n",
    "#     return: pd.DataFrame: dataframe of the top blogs and blogs\n",
    "#     \"\"\"\n",
    "#     url = \"https://www.findlaw.com/legalblogs/\"\n",
    "#     top_blogs = get_findlaw_top_blogs(url)\n",
    "#     blogs = get_findlaw_blogs(url)\n",
    "#     df = pd.concat([top_blogs, blogs], ignore_index=True)\n",
    "#     return df\n",
    "\n",
    "\n",
    "\n",
    "# df = concatinated_findlaw_blogs_link_index()\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_2 = get_findlaw_blogs('https://www.findlaw.com/legalblogs/')\n",
    "df_3 = get_findlaw_top_blogs('https://www.findlaw.com/legalblogs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.findlaw.com/legalblogs/law-and-life/',\n",
       " 'https://www.findlaw.com/legalblogs/personal-injury/',\n",
       " 'https://www.findlaw.com/legalblogs/small-business/',\n",
       " 'https://www.findlaw.com/legalblogs/greedy-associates/',\n",
       " 'https://www.findlaw.com/legalblogs/criminal-defense/',\n",
       " 'https://www.findlaw.com/legalblogs/technologist/',\n",
       " 'https://www.findlaw.com/legalblogs/estate-planning/',\n",
       " 'https://www.findlaw.com/legalblogs/findlaw-for-teens/',\n",
       " 'https://www.findlaw.com/legalblogs/celebrity-justice/',\n",
       " 'https://www.findlaw.com/legalblogs/legally-weird/',\n",
       " 'https://www.findlaw.com/legalblogs/courtside/',\n",
       " 'https://www.findlaw.com/legalblogs/consumer-protection/']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.findlaw.com/legalblogs/law-and-life/',\n",
       " 'https://www.findlaw.com/legalblogs/courtside/',\n",
       " 'https://www.findlaw.com/legalblogs/legally-weird/',\n",
       " 'https://www.findlaw.com/legalblogs/practice-of-law/',\n",
       " 'https://www.findlaw.com/legalblogs/federal-courts/',\n",
       " 'https://www.findlaw.com/lawyer-marketing/blog/']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'NoneType' object has no attribute 'text'\n",
      "An error occurred: 'NoneType' object has no attribute 'text'\n",
      "An error occurred: 'NoneType' object has no attribute 'text'\n",
      "An error occurred: 'NoneType' object has no attribute 'text'\n",
      "An error occurred: 'NoneType' object has no attribute 'text'\n",
      "An error occurred: 'NoneType' object has no attribute 'text'\n",
      "An error occurred: 'NoneType' object has no attribute 'text'\n",
      "An error occurred: 'NoneType' object has no attribute 'text'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def scrape_findlaw_blogs():\n",
    "    \"\"\"\n",
    "    Get the blogs from findlaw.com\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # links = df_3['link']\n",
    "        # link = 'https://www.findlaw.com/legalblogs/legally-weird/'\n",
    "        # driver.get(link)\n",
    "        # soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        # blogs = soup.find_all('div', class_='preview-column')\n",
    "        for link in df_3:\n",
    "            driver.get(link)\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            blogs = soup.find_all('div', class_= 'preview-content')\n",
    "                       \n",
    "            \n",
    "            blog_list = []\n",
    "\n",
    "            for blog in blogs:\n",
    "                title = blog.find('h2', class_='preview-title-blogs').text\n",
    "                author = blog.find('div', class_='authorByline').text\n",
    "                # date = blog.find('span', class_='date').text\n",
    "                link = blog.find('a')['href']\n",
    "                summary = blog.find('p', class_='preview-text').text\n",
    "                readmore = blog.find('a', class_='preview-button fl-button fl-link-button secondary')\n",
    "                readmore = readmore['href']\n",
    "                # print(readmore)\n",
    "                content = requests.get(readmore)\n",
    "                content = BeautifulSoup(content.text, 'html.parser')\n",
    "                content = content.find('div', class_='extra-row-spacing')\n",
    "                content = get_content(link)\n",
    "\n",
    "                \n",
    "                blog_list.append({\n",
    "                    'title': title,\n",
    "                    'author': author,\n",
    "                    # 'date': date,\n",
    "                    'link': link,\n",
    "                    'summary': summary,\n",
    "                    'content': content\n",
    "                })\n",
    "\n",
    "            df = pd.DataFrame(blog_list)\n",
    "            df.to_csv('findlaw_blogs.csv', index=False)\n",
    "\n",
    "        return blog_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_more_articles():\n",
    "    \"\"\"\n",
    "    Get more articles from findlaw.com and accumulate all pages' articles.\n",
    "    \"\"\"\n",
    "    articles_list = []  # Initialize outside the loop to accumulate all articles\n",
    "    try:\n",
    "        url = \"https://www.findlaw.com/legalblogs/law-and-life/\"\n",
    "        max_pages = 1680  # Adjust this value to scrape more pages\n",
    "        \n",
    "        for i in range(1, max_pages + 1):  # Include max_pages by adjusting the range\n",
    "            page_url = f\"{url}page/{i}/\"\n",
    "            driver.get(page_url)\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            articles_on_page = soup.find_all('div', class_='preview-content')\n",
    "            \n",
    "            for article in articles_on_page:\n",
    "                title = article.find('h2').text.strip()\n",
    "                link = article.find('a')['href']\n",
    "                author_element = article.find('div', class_='authorByline')\n",
    "                author = author_element.text.strip() if author_element else \"Unknown\"\n",
    "                summary_element = article.find('p', class_='preview-text')\n",
    "                summary = summary_element.text.strip() if summary_element else \"\"\n",
    "                content = get_content(link)\n",
    "                \n",
    "                articles_list.append({\n",
    "                    'title': title,\n",
    "                    'author': author,\n",
    "                    'link': link,\n",
    "                    'summary': summary,\n",
    "                    'content': content\n",
    "                })\n",
    "\n",
    "        # Save all articles to CSV after processing all pages\n",
    "        df = pd.DataFrame(articles_list)\n",
    "        df.to_csv('findlaw_more_articles.csv', index=False)\n",
    "        return articles_list\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_content(url):\n",
    "    \"\"\"\n",
    "    Get the content of the articles\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        content = soup.find('div', class_='g-col-12 g-col-md-10 g-start-md-2 g-col-xxl-9 g-start-xxl-1')\n",
    "        content = content.text\n",
    "        # print(content)\n",
    "        return content\n",
    "        \n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "url = \"https://www.findlaw.com/legalblogs/\"\n",
    "# get_content()\n",
    "# scrape_findlaw_blogs()\n",
    "get_more_articles()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
